# Evaluation codes for the MOL Bubi Challenge (2015)

Challenge page with the task descriptions:  https://dms.sztaki.hu/bubi

// Work in progress //

## Data files

### Raw data

We generated a small example dataset with 300 travels in 10 days between 3 fake stations ("A", "B" and "C").
The set has been cut into two parts: train (7 days: the first five consecutive days of the ten, plus two not consecutively) and test (3 days).

These are available under the "data/raw" folder.

### Solution files

A "solution file" is the reference, the ground truth, that contains data derived from the
"test" part of the raw data -- the information that has to be predicted by the challenge participants.

The sample solution file for task 1 is [data/task1-BRP/mini1-solution.csv](https://github.com/bubichallenge/challenge-codes/blob/master/data/task1-BRP/mini1-solution.csv), and for the second task: [data/task2-DSDP/mini2-solution.csv](https://github.com/bubichallenge/challenge-codes/blob/master/data/task2-DSDP/mini2-solution.csv).

To see how they have been created from the raw csv test data, see the end of this readme.


### Sample submission files

You can find them in the folders "data/task1-BRP" and "data/task2-DSDP".

---------

## Scripts

All scripts are written in Python (tested with Python 2.7.9, some of them won't work with Python3), 
and can be found in the "python" folder.

Tests can be run with [pytest](http://pytest.org "py.test"):

```bash
$ py.test
```

### "Real" evaluator scripts

These are used for evaluating the submission files that participants upload during the challenge.

#### For task 1 (Busiest Route Prediction, evaluated with [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)):

- [python/eval/ranking_evaluator.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/ranking_evaluator.py)
- [python/eval/ndcg.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/ndcg.py)

Note that the first script calls the second one.

#### For task 2 (Docking Station Demand Prediction, evaluated with [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)):

- [python/eval/decrease_evaluator.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/decrease_evaluator.py)

### Runner scripts

These are here just to run the evaluator codes scripts on the example data.

#### Evaluate the sample submissions for task 1

Run this:

```bash
$ python python/eval-mini-submissions-task1.py
```

The output (printed to the screen) will include daily NDCG values, and a summary (average) for each of the three days of the test dataset.


#### Evaluate the sample submissions for task 2

(W.I.P)

```bash
$ python python/eval-mini-submissions-task2.py
```


### Other scripts

To make it easier to understand what we maesure up to, we included scripts that create the solution files from the raw data containing the bicycle trips.
These are: 
- [python/preproc/compute_solution_1.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/preproc/compute_solution_1.py)
- [python/preproc/compute_solution_2.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/preproc/compute_solution_2.py)

The [sample solution file for task 1](https://github.com/bubichallenge/challenge-codes/blob/master/data/task1-BRP/mini1-solution.csv)
has been generated by running:

```bash
$ python python/preproc/compute_solution_1.py data/raw/mini_test.csv data/task1-BRP/mini1-solution.csv
```

Similarly, the small [solution file for task 2](https://github.com/bubichallenge/challenge-codes/blob/master/data/task2-DSDP/mini2-solution.csv) has been created this way:

```bash
python python/preproc/compute_solution_2.py data/raw/mini_test.csv data/task2-DSDP/mini2-solution.csv
```


