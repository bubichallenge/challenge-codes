# Evaluation codes for the MOL Bubi Challenge (2015)

Probably you'll want to check the [MOL Bubi Challenge page](https://dms.sztaki.hu/bubi) at first, it has the task descriptions.

## Data files

### Raw data

We generated a small example dataset with 300 travels in 10 days between three fake stations ("A", "B" and "C").
The set has been cut into two parts: train (7 days: the first five consecutive days of the ten, plus two not consecutively) and test (3 days).

These are available under the [data/raw](https://github.com/bubichallenge/challenge-codes/tree/master/data/raw) folder.

### Solution files

A _solution file_ is the reference, the ground truth, that contains data derived from the
"test" part of the raw data -- the information that has to be predicted by the challenge participants.

The sample solution file for task 1 is [data/task1-BRP/mini1-solution.csv](https://github.com/bubichallenge/challenge-codes/blob/master/data/task1-BRP/mini1-solution.csv), and for the second task: [data/task2-DSDP/mini2-solution.csv](https://github.com/bubichallenge/challenge-codes/blob/master/data/task2-DSDP/mini2-solution.csv).

To see how they have been created from the raw csv test data, see the end of this readme ([Other scripts](###Other scripts)).


### Sample submission files

These were edited by hand, to provide some small examples. You can find them in the folders [data/task1-BRP](https://github.com/bubichallenge/challenge-codes/tree/master/data/task1-BRP) and [data/task2-DSDP](https://github.com/bubichallenge/challenge-codes/tree/master/data/task2-DSDP).

---------

## Scripts

All scripts are written in Python (tested with Python 2.7.9, some of them won't work with Python3), 
and can be found in the [python](https://github.com/bubichallenge/challenge-codes/tree/master/python) folder.

Tests can be run with [pytest](http://pytest.org "py.test"):

```bash
$ py.test
```

### "Real" evaluator scripts

These are used for evaluating the submission files that participants upload during the challenge.

#### For task 1 (Busiest Route Prediction, evaluated with [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)):

- [python/eval/ranking_evaluator.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/ranking_evaluator.py)
- [python/eval/ndcg.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/ndcg.py)

Note that the first script calls the second one.

#### For task 2 (Docking Station Demand Prediction, evaluated with [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)):

- [python/eval/decrease_evaluator.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/eval/decrease_evaluator.py)

### Runner scripts

These are here just to run the evaluator codes scripts on the example data.

#### Evaluate the sample submissions for task 1

Run this:

```bash
$ python python/eval-mini-submissions-task1.py
```

The output (printed to the screen) will include daily NDCG values, and a summary (average) for each of the three days of the test dataset.


#### Evaluate the sample submissions for task 2

```bash
$ python python/eval-mini-submissions-task2.py
```
When running the above command, you should see the RMSE values of the sample submission files printed to the console screen.

### Other scripts

To make it easier to understand what the prediction tasks are exactly, we included scripts that create the perfect solution files from the raw data containing the bicycle trips of the days of the 'test' subset ('test' subset meaning the non-training days, about which we want to make predictions).
These scripts are: 
- [python/preproc/compute_solution_1.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/preproc/compute_solution_1.py)
- [python/preproc/compute_solution_2.py](https://github.com/bubichallenge/challenge-codes/blob/master/python/preproc/compute_solution_2.py)

The [sample solution file for task 1](https://github.com/bubichallenge/challenge-codes/blob/master/data/task1-BRP/mini1-solution.csv)
has been generated by running:

```bash
$ python python/preproc/compute_solution_1.py data/raw/mini_test.csv data/task1-BRP/mini1-solution.csv
```

Similarly, the small [solution file for task 2](https://github.com/bubichallenge/challenge-codes/blob/master/data/task2-DSDP/mini2-solution.csv) has been created this way:

```bash
python python/preproc/compute_solution_2.py data/raw/mini_test.csv data/task2-DSDP/mini2-solution.csv
```

These scripts ('compute_solution_x.py) were used to generate the real solution files, against which the team submissions will be evaluated.
